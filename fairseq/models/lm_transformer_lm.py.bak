# This file is copied from fairseq/models/transformer.py
# and edit in it
# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

import math
from typing import Any, Dict, List, Optional, Tuple

import torch
import torch.nn as nn
from fairseq import utils
from fairseq.models import (
    BaseFairseqModel,
    FairseqEncoder,
    FairseqEncoderDecoderModel,
    FairseqIncrementalDecoder,
    roberta,
    bart,
    transformer,
    transformer_lm,
    register_model,
    register_model_architecture,
)
from fairseq.modules import (
    AdaptiveSoftmax,
    FairseqDropout,
    LayerDropModuleList,
    LayerNorm,
    PositionalEmbedding,
    SinusoidalPositionalEmbedding,
    TransformerDecoderLayer,
    TransformerEncoderLayer,
)
#from fairseq.modules.checkpoint_activations import checkpoint_wrapper
from fairseq.modules.quant_noise import quant_noise as apply_quant_noise_
from torch import Tensor


DEFAULT_MAX_SOURCE_POSITIONS = 1024
DEFAULT_MAX_TARGET_POSITIONS = 1024

@register_model("lm_transformer_lm")
class LMTransformerLMModel(BaseFairseqModel):
    """ This class is a general class for using language model before and after main model"""

    def __init__(self, args, lm, encoder, decoder):
        self.lm1=FairseqEncoder(args, lm)
        self.main_model=transformer.TransformerModel(args, encoder, decoder)
        self.lm2=FairseqEncoder(args, lm)

    @staticmethod
    def add_args(parser):
            # specific model arguments
        parser.add_argument('--dropout',
            help="Dropout probability",
        )
        parser.add_argument("--min-lr",
            help="minimum allowed value for learning rate",
            default=0,
        )
        parser.add_argument("--model-encoder-layers", type=int, metavar='N',
            help="number of layers of main model encoder",
        )
        parser.add_argument("--model-encoder-embed-dim", type=int, metavar='N',
            help="embeddings dimension in main model encoder layers",
        )
        parser.add_argument("--model-encoder-attention-heads", type=int, metavar='N',
            help="number of attention heads in main model encoder",
        )
        parser.add_argument("--model-encoder-ffn-embed-dim", type=int, metavar='N',
            help="main model encoder embedding dimension for FFN",
        )
        parser.add_argument("--model-decoder-layers", type=int, metavar='N',
            help="number of layers of decoder of main model",
        )
        parser.add_argument("--model-decoder-embed-dim", type=int, metavar='N',
            help="embeddings dimension in main model decoder layers",
        )
        parser.add_argument("--model-decoder-attention-heads", type=int, metavar='N',
            help="number of attention heads in main model decoder",
        )
        parser.add_argument("--model-decoder-ffn-embed-dim", type=int, metavar='N',
            help="main model decoder embedding dimension for FFN",
        )

        parser.add_argument("--share-model-decoder-input-output-embed",
            help="share decoder input and output embeddings",
        )
        parser.add_argument("--share-all-embeddings", action='store_true',
        help="share main model encoder, decoder and output embeddings",
        )

    @classmethod
    def build_model(cls, args, task):
        #transformer=transformer.TransformerModel(args, FairseqEncoder(args, lm), FairseqDecoder(args, lm))
        #cls_lm1, encoder_lm1=FairseqEncoder(args, lm).build_model(cls, args, task)
        #cls_lm1, encoder_lm1=cls.lm1.build_model(cls, args, task)
        #cls_model, args_model, encoder_model, decoder_model= transformer.build_model(cls, args, task)
        #cls_model, args_model, encoder_model, decoder_model= transformer.TransformerModel(args, FairseqEncoder(args, lm), FairseqDecoder(args, lm))
        #cls_lm2, encoder_lm2=FairseqEncoder(args, lm).build_model(cls, args, task)
        lm1=cls.build_language_model(args, task)
        main_model=cls.build_main_model(args, task)
        main_model_enc, main_model_dec=main_model[0], main_model[1]
        lm2=cls.build_language_model(args, task)
        return (cls, args, lm1, main_model, lm2)


    @classmethod
    def build_main_model(cls, args, task):
        # This function should return the correct model regardless of it is transformer or not.
        #return transformer.TransformerModel.build_model(args, task)
	args.encoder_embed_path=args.decoder_embed_path=None
        encoder_embed_tokens=transformer.TransformerModel.build_embedding(args, task.source_dictionary, args.model_encoder_embed_dim, args.encoder_embed_path)
        decoder_embed_tokens=transformer.TransformerModel.build_embedding(args, task.target_dictionary, args.model_decoder_embed_dim, args.decoder_embed_path)
        enc=transformer.TransformerModel.build_encoder(args, task.source_dictionary, encoder_embed_tokens)
        dec=transformer.TransformerModel.build_decoder(args, task.target_dictionary, decoder_embed_tokens)
        return (enc, dec)

    @classmethod
    def build_language_model(cls, args, task):
        return  transformer_lm.TransformerLanguageModel.build_model(args, task)

@register_model_architecture("lm_transformer_lm", "lm_transformer_lm")
def base_architecture(args):
    args.src_lm=getattr(args, "src_lm", None)
    args.tgt_lm=getattr(args, "tgt_lm", None)
    args.dropout=getattr(args, 'dropout', 0.1)
